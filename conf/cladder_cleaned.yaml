general:
  llms: [gpt3.04, gpt3.041, gpt3.042, gpt3.043, gpt3.5, gpt4_1106_cot, gpt4_1106, llama007]
  num_classes: 2
  results_path: results # Root folder to save the trained ngram-MLPs to within which the timestamped folder will live

instructions:
  do_preprocessing: 1
  do_training: 1
  do_explaining: 0

preprocessing:
  dataset_path: datasets/cladder/cladder_clean.pkl
  ngram_range: [1, 2]
  top_k_ngrams: 20000
  token_mode: word
  min_document_frequency: 2
  split_ratio: 0.2

training:
  learning_rate: 3.e-4
  epochs: 100
  batch_size: 128
  layers: 2
  units: 64
  dropout_rate: 0.9

explaining:
  strats: [rung, query_type, sensical, phenomenon]
  plot_type: violin
  num_background_points: 200
  seed: 10
  max_ngram_display: 20